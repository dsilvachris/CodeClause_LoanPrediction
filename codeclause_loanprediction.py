# -*- coding: utf-8 -*-
"""CodeClause_LoanPrediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fVExgbAxlr8S_AJ5WmYHrcJnolzbxLEQ

**CodeClause Loan Prediction Project**

Importing the Python libraries we will be using during this project.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

"""Importing and reading the data 
For this project we have 2 datasets(Train and Test)
"""

train = pd.read_csv('/content/drive/MyDrive/train_u6lujuX_CVtuZ9i.csv')
train.head()

test = pd.read_csv('/content/drive/MyDrive/test_Y3wMUE5_7gLdaTN.csv')
test.head()

train_og = train.copy()
test_og = test.copy()

train.drop('Loan_ID',axis=1,inplace=True)
test.drop('Loan_ID',axis=1,inplace=True)

train.columns

"""We have 12 independent variables and 1 target variable, i.e. Loan_Status in the training dataset."""

test.columns

"""We have similar features in the test dataset as the training dataset except for the Loan_Status.
So now we will predict the Loan_Status using the model which we will be building for this project using the train data
"""

train.dtypes

train.shape

"""We have 614 rows and 13 columns in the train dataset."""

test.shape

"""We have 367 rows and 12 columns in test dataset"""

train['Loan_Status'].value_counts()

"""Setting the Normalize values to True to print proportions instead of number"""

from matplotlib.colors import Normalize
train['Loan_Status'].value_counts(normalize = True)

train['Loan_Status'].value_counts().plot.bar()

"""The loan of 422(around 69%) people out of 614 were approved.

To vitualize each variable seperatly we catorize variables as Categorical, Ordinal and Numerical.

**Categorical** i.e Gender, Married, Self_Employed, Credit_History, Loan_Status
"""

train['Gender'].value_counts(normalize=True).plot.bar(title='Gender')
plt.show()

train['Married'].value_counts(normalize=True).plot.bar(title='Married')
plt.show()

train['Self_Employed'].value_counts(normalize=True).plot.bar(title='Self_Employed')
plt.show()

train['Credit_History'].value_counts(normalize=True).plot.bar(title='Credit_History')
plt.show()

"""From the above 4 bar plots we can conclude that


1.  80% of applicants in the dataset are male.
2.  Around 65% of the applicants in the dataset are married.
3. Around 15% of applicants in the dataset are self-employed.
4. Around 85% of applicants have repaid their doubts.

**Ordinal**i.e Dependents, Education, Property_Area
"""

train['Dependents'].value_counts(normalize=True).plot.bar(title='Dependents')
plt.show()

train['Education'].value_counts(normalize=True).plot.bar(title='Education')
plt.show()

train['Property_Area'].value_counts(normalize=True).plot.bar(title='Property_Area')
plt.show()

"""From the above 3 bar plots we can conclude that


1.   Most of the applicants don't have any dependents.
2.   Around 80% of the applicants are Graduate
3.   Most of the applicants are from the Semiurban area

Numerical i.e Applicient Income
"""

import seaborn as sns
sns.distplot(train['ApplicantIncome'])
plt.show()
train['ApplicantIncome'].plot.box()
plt.show()

train.boxplot(column='ApplicantIncome', by = 'Education') 
plt.suptitle("")

"""From the above figures we can see that there are a higher number of graduates with very high incomes, which are appearing to be outliers."""

sns.distplot(train['CoapplicantIncome'])
plt.show()
train['CoapplicantIncome'].plot.box()
plt.show()

"""We see a similar distribution as that of the applicant's income. The majority of co-applicants income ranges from 0 to 5000. We also see a lot of outliers in the applicant's income and it is not normally distributed."""

train.notna()
sns.distplot(train['LoanAmount'])
plt.show()
train['LoanAmount'].plot.box()
plt.show()

"""**Bivariate Analysis**

Categorical Independent Variable vs Target Variable
"""

Gender=pd.crosstab(train['Gender'],train['Loan_Status'])
Gender.div(Gender.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True,figsize=(4,4))
plt.show()
Married=pd.crosstab(train['Married'],train['Loan_Status'])
Dependents=pd.crosstab(train['Dependents'],train['Loan_Status'])
Education=pd.crosstab(train['Education'],train['Loan_Status'])
Self_Employed=pd.crosstab(train['Self_Employed'],train['Loan_Status'])
Married.div(Married.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True,figsize=(4,4))
plt.show()
Dependents.div(Dependents.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True,figsize=(4,4))
plt.show()
Education.div(Education.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True,figsize=(4,4))
plt.show()
Self_Employed.div(Self_Employed.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True,figsize=(4,4))
plt.show()

"""From the above diagrams we can see that


1.   It can be inferred that the proportion of male and female applicants is   more or less the same for both approved and unapproved loans.
2.   The proportion of married applicants is higher for approved loans.
3.   Distribution of applicants with 1 or 3+ dependents is similar across both the categories of Loan_Status.
4.  There is nothing significant we can infer from Self_Employed vs Loan_Status plot.


"""

Credit_History=pd.crosstab(train['Credit_History'],train['Loan_Status'])
Property_Area=pd.crosstab(train['Property_Area'],train['Loan_Status'])
Credit_History.div(Credit_History.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True, figsize=(4,4))
plt.show()
Property_Area.div(Property_Area.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True, figsize=(4,4))
plt.show()

"""From the above figures we can see that

1.   It seems people with a credit history as 1 are more likely to get their loans approved.
2.   The proportion of loans getting approved in the semi-urban area is higher as compared to that in rural or urban areas.

Numerical Independent Variable vs Target Variable
"""

train.groupby('Loan_Status')['ApplicantIncome'].mean().plot.bar(figsize=(4,4))

"""Here the y-axis represents the mean applicant income. We don’t see any change in the mean income. So, let’s make bins for the applicant income variable based on the values in it and analyze the corresponding loan status for each bin."""

bins=[0,2500,4000,6000,81000]
group=['Low','Average','High','Very high']
train['Income_bin']=pd.cut(train['ApplicantIncome'],bins,labels=group)
Income_bin=pd.crosstab(train['Income_bin'],train['Loan_Status'])
Income_bin.div(Income_bin.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True)
plt.xlabel('ApplicantIncome')
P=plt.ylabel('Percentage')

bins=[0,1000,3000,42000]
group=['Low','Average','High']
train['Coapplicant_Income_bin']=pd.cut(train['CoapplicantIncome'],bins,labels=group)
Coapplicant_Income_bin=pd.crosstab(train['Coapplicant_Income_bin'],train['Loan_Status'])
Coapplicant_Income_bin.div(Coapplicant_Income_bin.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True)
plt.xlabel("CoapplicantIncome")
P=plt.ylabel("Percentage")

"""From the above diagrams we can say
1. It can be inferred that Applicant's income does not affect the chances of loan approval which contradicts our hypothesis in which we assumed that if the applicant's income is high the chances of loan approval will also be high.
2. It shows that if co-applicants income is less the chances of loan approval are high. But this does not look right. The possible reason behind this may be that most of the applicants don’t have any co-applicant so the co-applicant income for such applicants is 0 and hence the loan approval is not dependent on it. So, we can make a new variable in which we will combine the applicant’s and co-applicants income to visualize the combined effect of income on loan approval.

Combining the Applicant Income and Co-applicant Income to get the combined effect of Total Income on the Loan_Status.
"""

train['Total_Income']=train['ApplicantIncome']+train['CoapplicantIncome']
bins=[0,2500,4000,6000,81000]
group=['Low','Average','High','Very high']
train['Total_Income_bin']=pd.cut(train['Total_Income'],bins,labels=group)
Total_Income_bin=pd.crosstab(train['Total_Income_bin'],train['Loan_Status'])
Total_Income_bin.div(Total_Income_bin.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True)
plt.xlabel('Total_Income')
P=plt.ylabel('Percentage')

bins=[0,100,200,700]
group=['Low','Average','High']
train['LoanAmount_bin']=pd.cut(train['LoanAmount'],bins,labels=group)
LoanAmount_bin=pd.crosstab(train['LoanAmount_bin'],train['Loan_Status'])
LoanAmount_bin.div(LoanAmount_bin.sum(1).astype(float), axis=0).plot(kind="bar",stacked=True)
plt.xlabel('LoanAmount')
P=plt.ylabel('Percentage')

"""From the above diagrams we can say

1.   We can see that Proportion of loans getting approved for applicants having low Total_Income is very less compared to that of applicants with Average, High & Very High Income.
2.  It can be seen that the proportion of approved loans is higher for Low and Average Loan Amount as compared to that of High Loan Amount which supports our hypothesis in which we considered that the chances of loan approval will be high when the loan amount is less..

Now lets drop the bins which we created for the exploration part. We will change the 3+ in dependents variable to 3 to make it a numerical variable. We will also convert the target variable’s categories into 0 and 1 so that we can find its correlation with numerical variables. One more reason to do so is few models like logistic regression takes only numeric values as input. We will replace N with 0 and Y with 1.
"""

train=train.drop(['Income_bin', 'Coapplicant_Income_bin', 'LoanAmount_bin', 'Total_Income_bin', 'Total_Income'], axis=1)
train['Dependents'].replace('3+', 3,inplace=True)
test['Dependents'].replace('3+', 3,inplace=True)
train['Loan_Status'].replace('N', 0,inplace=True)
train['Loan_Status'].replace('Y', 1,inplace=True)

matrix = train.corr()
f, ax = plt.subplots(figsize=(5,5))
sns.heatmap(matrix,vmax=.8,square=True,cmap="BuPu", annot = True)

"""**Missing Values**"""

train.isnull().sum().sort_values(ascending=False)

print("Before filling missing values\n\n","#"*50,"\n")
null_cols = ['Credit_History', 'Self_Employed', 'LoanAmount','Dependents', 'Loan_Amount_Term', 'Gender', 'Married']


for col in null_cols:
    print(f"{col}:\n{train[col].value_counts()}\n","-"*50)
    train[col] = train[col].fillna(
    train[col].dropna().mode().values[0] )   

    
train.isnull().sum().sort_values(ascending=False)
print("After filling missing values\n\n","#"*50,"\n")
for col in null_cols:
    print(f"\n{col}:\n{train[col].value_counts()}\n","-"*50)

#converting categorical values to numbers

to_numeric = {'Male': 1, 'Female': 2,
'Yes': 1, 'No': 2,
'Graduate': 1, 'Not Graduate': 2,
'Urban': 3, 'Semiurban': 2,'Rural': 1,
'Y': 1, 'N': 0,
'3+': 3}

# adding the new numeric values from the to_numeric variable to both datasets
train = train.applymap(lambda lable: to_numeric.get(lable) if lable in to_numeric else lable)
test = test.applymap(lambda lable: to_numeric.get(lable) if lable in to_numeric else lable)

# convertind the Dependents column
Dependents_ = pd.to_numeric(train.Dependents)
Dependents__ = pd.to_numeric(test.Dependents)

# dropping the previous Dependents column
train.drop(['Dependents'], axis = 1, inplace = True)
test.drop(['Dependents'], axis = 1, inplace = True)

# concatination of the new Dependents column with both datasets
train = pd.concat([train, Dependents_], axis = 1)
test = pd.concat([test, Dependents__], axis = 1)

# checking the our manipulated dataset for validation
print(f"training set (row, col): {train.shape}\n\ntesting set (row, col): {test.shape}\n")
print(train.info(), "\n\n", test.info())

"""Dividing the dataset into training and testing using **train_test_split**"""

from sklearn.model_selection import train_test_split
y = train['Loan_Status']
X = train.drop('Loan_Status', axis = 1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

"""Model 1: **DecisionTree**"""

from sklearn.tree import DecisionTreeClassifier
DT = DecisionTreeClassifier()
DT.fit(X_train, y_train)

y_predict = DT.predict(X_test)

#  prediction Summary by species
from sklearn.metrics import classification_report
print(classification_report(y_test, y_predict))

# Accuracy score
from sklearn.metrics import accuracy_score
DT_SC = accuracy_score(y_predict,y_test)
print(f"{round(DT_SC*100,2)}% Accurate")

"""Model 2: **Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression()
LR.fit(X_train, y_train)

y_predict = LR.predict(X_test)

#  prediction Summary by species
print(classification_report(y_test, y_predict))

# Accuracy score
LR_SC = accuracy_score(y_predict,y_test)
print('accuracy is',accuracy_score(y_predict,y_test))

"""Model 3: **xgboost**"""

from xgboost import XGBClassifier
XGB = XGBClassifier()
XGB.fit(X_train, y_train)

y_predict = XGB.predict(X_test)

#  prediction Summary by species
print(classification_report(y_test, y_predict))

# Accuracy score
XGB_SC = accuracy_score(y_predict,y_test)
print(f"{round(XGB_SC*100,2)}% Accurate")

"""**Conclusion**"""

score = [DT_SC,XGB_SC,LR_SC]
Models = pd.DataFrame({
    'n_neighbors': ["Decision Tree","XGBoost", "Logistic Regression"],
    'Score': score})
Models.sort_values(by='Score', ascending=False)